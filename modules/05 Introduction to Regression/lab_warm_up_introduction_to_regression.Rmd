---
title: "Lab Warm-up: Introduction to Regression"
output: html_notebook
---

# Overview

In this lab warm-up, we will review some of the basics of regression analysis, and the R code we need to write to fit regression models to our data. Specifically, we will review three different regression models - linear, logistic, and Poisson.

Linear regression, logistic regression, and Poisson regression are all specific cases of a group of regression models called **Generalized Linear Models (GLMs)**. In R, we fit GLMs with the `glm()` function. In general, we will need to pass the following the following information to the `glm()` function:

```{r eval=FALSE}
glm(
  # The regression formula Y ~ X
  # The distribution family and link function
  # The data frame
)
```

The formula we pass to the `glm()` function takes the form `Y ~ X`, where `Y` is the outcome variable and `X` is the predictor variable or variables.

In this module, we will use one of the following distribution family and link functions:

| Regression Model | Distribution Family |   Link Function   |
|:----------------:|:-------------------:|:-----------------:|
|      Linear      |     gaussian()      | link = "identity" |
|     Logistic     |     binomial()      |  link = "logit"   |
|     Poisson      |      poisson()      |   link = "log"    |

And finally, we will pass the name of a data frame to the `data` argument of the `glm()` function.

Although not strictly necessary, we will also use the `broom` package to to make the results from the `glm()` function a little easier to work with.


# Load packages

```{r}
library(dplyr, warn.conflicts = FALSE)
library(broom)
library(ggplot2)
library(meantables)
library(freqtables)
```


# Model syntax and interpretation

The introduction to R4Epi says that our philosophy is "to start each concept by showing you the end result and then deconstruct how we arrived at that result, where possible. We find that it is easier for many people to understand new concepts when learning them as a component of a final product." In that  spirit, this section will briefly cover the R syntax we will use to fit linear, logistic, and Poisson regression models. We will also briefly cover interpretation of the results.

Before we can fit any models to our data, we need to have some data. Therefore, we will simulate some very simple data below that we can use for illustrative purposes.

```{r}
set.seed(123)
n <- 20
df <- tibble(
  x_cont  = rnorm(n, 10, 1),
  x_cat   = sample(0:1, n, TRUE, c(.7, .3)),
  y_cont  = if_else(
    x_cat == 0, 
    x_cont + rnorm(n, 1, 0.1), 
    x_cont - rnorm(n, 1, 0.1)
  ),
  y_cat   = if_else(
    x_cat == 0,
    sample(0:1, n, TRUE, c(.9, .1)),
    sample(0:1, n, TRUE, c(.5, .5))
  ),
  y_count = if_else(
    x_cat == 0,
    sample(1:5, n, TRUE, c(.1, .2, .3, .2, .2)),
    sample(1:5, n, TRUE, c(.3, .3, .2, .1, .1))
  )
)

df
```

## Linear regression

Now that we have some simulated data, we will fit our models. We will start with linear models.

### Continuous regressand continuous regressor

```{r}
glm(
  y_cont ~ x_cont,                      # Formula
  family = gaussian(link = "identity"), # Family/distribution/Link function
  data   = df                           # Data
)
```

#### Interpretation

1. Intercept: The mean value of `y_cont` when `x_cont` is equal to zero. 

2. x_cont: The average change in `y_cont` for each one-unit increase in `x_cont`. 

### Continuous regressand categorical regressor

```{r}
glm(
  y_cont ~ x_cat,                       # Formula
  family = gaussian(link = "identity"), # Family/distribution/Link function
  data   = df                           # Data
)
```

#### Interpretation

1. Intercept: The mean value of `y_cont` when `x_cat` is equal to zero. 

2. x_cat: The change in the mean value of `y_cont` when `x_cat` changes from zero to one. 

## Logistic regression

Now, let's take a look at a couple of logistic models.

### Categorical regressand continuous regressor

```{r}
glm(
  y_cat  ~ x_cont,                   # Formula
  family = binomial(link = "logit"), # Family/distribution/Link function
  data   = df                        # Data
)
```

#### Interpretation

1. Intercept: The natural log of the odds of `y_cat` when `x_cont` is equal to zero. 

2. x_cont: The average change in the natural log of the odds of `y_cont` for each one-unit increase in `x_cont`. 

### Categorical regressand categorical regressor

```{r}
glm(
  y_cat  ~ x_cat,                    # Formula
  family = binomial(link = "logit"), # Family/distribution/Link function
  data   = df                        # Data
)
```

#### Interpretation

1. Intercept: The natural log of the odds of `y_cat` when `x_cat` is equal to zero. 

2. x_catt: The average change in the natural log of the odds of `y_cont` when `x_cat` changes from zero to one.

## Poisson regression

Finally, let's take a look at a couple of Poisson models.

### Count regressand continuous regressor

```{r}
glm(
  y_count  ~ x_cont,                # Formula
  family   = poisson(link = "log"), # Family/distribution/Link function
  data     = df                     # Data
)
```

#### Interpretation

1. Intercept: The natural log of the mean of `y_count` when `x_cont` is equal to zero. 

2. x_cont: The average change in the log of the mean of `y_cont` for each one-unit increase in `x_cont`. 

### Count regressand categorical regressor

```{r}
glm(
  y_count  ~ x_cat,                 # Formula
  family   = poisson(link = "log"), # Family/distribution/Link function
  data     = df                     # Data
)
```

#### Interpretation

1. Intercept: The natural log of the mean of `y_count` when `x_cat` is equal to zero. 

2. x_cat: The average change in the natural log of the mean of `y_count` when `x_cat` changes from zero to one.


# Regression Intuition

In the sections above, we got a glimpse into the syntax we will need to use to perform linear, logistic, and Poisson regression in R. However, the syntax alone is not very useful if we don't understand what's happening in a more intuitive way. The the sections that follow, that's exactly what we will try to develop. 

## Diabetes and family history

```{r}
diabetes <- tidyr::expand_grid(
  family_history = c(1, 0),
  diabetes       = c(1, 0)
) %>% 
  mutate(count = c(58, 196, 63, 381)) %>% 
  tidyr::uncount(count)

diabetes
```

Remember, in the frequintist view, the regression of a variable Y on another variable X is the function that describes how the average (mean) value of Y changes across population subgroups defined by values of X. (Lash TL, VanderWeel TJ, Haneuse S, Rothman KJ. Modern Epidemiology. fourth. Wolters Kluwer; 2021)

The mean value of diabetes across subgroups defined by family history are: 

```{r}
diabetes %>% 
  group_by(family_history) %>% 
  mean_table(diabetes)
```

```{r}
diabetes %>% 
  freq_table(family_history, diabetes) %>% 
  select(row_var:n, percent_row)
```

Take a look at the proportion of people with family history of diabetes who have diabetes and the proportion of people without family history of diabetes who have diabetes. You should notice that it is the same as the subgroup means. 

This illustrates an important property: "When the regressand Y is a binary indicator (0, 1) variable, E(Y|X=x) is called a binary regression, and this regression simplifies in a very useful manner. Specifically, when Y can be only 0 or 1 the average E(Y|X=x) equals the proportion of population members who have Y=1 among those who have X=x."

Now, let's fit a linear regression model to this data.

## Linear regression

```{r}
glm(
  diabetes ~ family_history,
  family = gaussian(link = "identity"),
  data   = diabetes
)
```

1. Intercept: The mean value of `diabetes` when `family_history` is equal to zero. 

2. family_history: The average change in `diabetes` for each one-unit increase in `family_history`.

0.14189 + 0.08645

Hopefully, this illustration has given increased your comfort level with regression analysis. It's just an extension of the simpler methods we previously learned about. However, these same concepts extend to multivariable (not to be confused with multivariate) regression. This is one of the primary reasons we use regression. Imagine how messy the mean tables and/or frequency tables would get if we had 10 regressor variables instead of one.

However, we will not typically use linear regression with binary outcomes as we did above. This was only meant to give you a feel for what regression is doing under the hood. Now, let's take a look at a more realistic example using a continuous outcome. 

### Deep abdominal adipose tissue and waist circumference.

```{r}
set.seed(123)
n <- 100
daat_waist <- tibble(
  waist    = rnorm(n, 39, 2),
  waist_c  = waist - 39,
  beta1    = rnorm(n, 3.45, 0.2),
  daat_bar = rnorm(n, 101, 8),
  daat     = daat_bar + (beta1 * waist_c)
)
daat_waist
```

What is the average waist circumference?

```{r}
mean(daat_waist$waist)
```

What is the average deep abdominal adipose tissue?

```{r}
mean(daat_waist$daat)
```

What is the relationship between deep abdominal adipose tissue and waist circumference?

```{r}
ggplot(daat_waist, aes(x = waist, y = daat)) +
  geom_point()
```

```{r}
ggplot(daat_waist, aes(x = waist, y = daat)) +
  geom_point() +
  geom_hline(yintercept = mean(daat_waist$daat), color = "red") 
```

```{r}
cor.test(daat_waist$daat, daat_waist$waist)
```

```{r}
ggplot(daat_waist, aes(x = waist, y = daat)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) 
```

What is the relationship between deep abdominal adipose tissue (regressand) and waist circumference (regressor)?

```{r}
glm(
  daat ~ waist,
  family = gaussian(link = "identity"),
  data   = daat_waist
)
```

#### Interpretation

1. Intercept: The mean value of deep abdominal adipose tissue when waist circumference is 0 is -9.580. 

2. waist: The average change in deep abdominal adipose tissue for each one inch increase in waist circumference is 2.86. 

```{r}
ggplot(daat_waist, aes(x = waist, y = daat)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE) + 
  geom_abline(intercept = -9.580, slope = 2.862, color = "red", linetype = "dashed") 
```

### Categorical regressor (large waist)

```{r}
daat_waist <- daat_waist %>% 
  mutate(
    large_waist   = if_else(waist < mean(waist), 0, 1),
    large_waist_f = factor(large_waist, 0:1, c("No", "Yes"))
  )
daat_waist
```

```{r}
daat_by_large_f <- daat_waist %>% 
  group_by(large_waist_f) %>% 
  mean_table(daat)

daat_by_large_f
```

```{r}
ggplot(daat_waist, aes(x = large_waist_f, y = daat)) +
  geom_point() +
  geom_segment(
    aes(x = c(0.75, 1.75), y = mean, xend = c(1.25, 2.25), yend = mean), 
    size = 1.5, color = "red", data = daat_by_large_f
  )
```

What is the relationship between deep abdominal adipose tissue (regressand) and tall (regressor)?

```{r}
glm(
  daat ~ large_waist_f,
  family = gaussian(link = "identity"),
  data   = daat_waist
)
```

#### Interpretation

1. Intercept: The mean value of deep abdominal adipose tissue among people who are not tall is equal to 98.228. 

2. large_waist_fYes: The average change in deep abdominal adipose tissue for people with a large waste circumference compared to people with a small waste circumference is 8.854.

98.228 + 8.854


# Logistic regression

## Elder mistreatment

```{r}
set.seed(123)
n <- 100
em_dementia <- tibble(
  age = sample(65:100, n, TRUE),
  dementia = case_when(
    age < 70 ~ sample(0:1, n, TRUE, c(.99, .01)),
    age < 75 ~ sample(0:1, n, TRUE, c(.97, .03)),
    age < 80 ~ sample(0:1, n, TRUE, c(.91, .09)),
    age < 85 ~ sample(0:1, n, TRUE, c(.84, .16)),
    age < 90 ~ sample(0:1, n, TRUE, c(.78, .22)),
    TRUE ~ sample(0:1, n, TRUE, c(.67, .33))
  ),
  dementia_f = factor(dementia, 0:1, c("No", "Yes")),
  em = case_when(
    dementia_f == "No"  ~ sample(0:1, n, TRUE, c(.9, .1)),
    dementia_f == "Yes" ~ sample(0:1, n, TRUE, c(.5, .5))
  ),
  em_f = factor(em, 0:1, c("No", "Yes"))
)
em_dementia
```

### Categorical regressor (dementia)

```{r}
em_dementia %>% 
  freq_table(dementia_f, em_f) %>% 
  select(row_var:n_row, percent_row)
```

```{r}
glm(
  em_f  ~ dementia_f,
  family = binomial(link = "logit"),
  data   = em_dementia
)
```

#### Interpretation

1. Intercept: The natural log of the odds of `elder mistreatment` among people who do not have dementia is -2.734. (Generally not of great interest or directly interpreted)

2. dementia_fYes: The average change in the natural log of the odds of `elder mistreatment` among people with dementia compared to people without dementia.

```{r}
tibble(
  intercpet   = exp(-2.1734),
  dementia_or = exp(2.282)
)
```

3. dementia_fYes: People with dementia have 9.8 times the odds of `elder mistreatment` among compared to people without dementia.

```{r}
em_dementia_ct <- matrix(
  c(a = 7, b = 11, c = 5, d = 77),
  ncol = 2,
  byrow = TRUE
)

em_dementia_ct <- addmargins(em_dementia_ct)

dimnames(em_dementia_ct) <- list(
  Dementia = c("Yes", "No", "col_sum"), # Row names
  EM       = c("Yes", "No", "row_sum")  # Then column names
)

em_dementia_ct
```

```{r}
incidence_prop <- em_dementia_ct[, "Yes"] / em_dementia_ct[, "row_sum"]
em_dementia_ct <- cbind(em_dementia_ct, incidence_prop)
em_dementia_ct
```

```{r}
incidence_odds <- em_dementia_ct[, "incidence_prop"] / (1 - em_dementia_ct[, "incidence_prop"])
em_dementia_ct <- cbind(em_dementia_ct, incidence_odds)
em_dementia_ct
```

```{r}
ior <- em_dementia_ct["Yes", "incidence_odds"] / em_dementia_ct["No", "incidence_odds"]
ior
```

```{r}
# Alternative coding
em_dementia %>% 
  freq_table(dementia_f, em_f) %>% 
  select(row_var:n_row, percent_row) %>% 
  filter(col_cat == "Yes") %>% 
  mutate(
    prop_row = percent_row / 100,
    odds = prop_row / (1 - prop_row)
  ) %>% 
  summarise(or = odds[row_cat == "Yes"] / odds[row_cat == "No"])
```

### Continuous regressor (age)

```{r}
ggplot(em_dementia, aes(x = age, y = em_f)) +
  geom_point()
```

```{r}
em_dementia %>%
  group_by(em_f) %>% 
  mean_table(age)
```

```{r}
glm(
  em_f  ~ age,
  family = binomial(link = "logit"),
  data   = em_dementia
)
```

#### Interpretation

1. Intercept: The natural log of the odds of `elder mistreatment` among people who do not have dementia is -3.80. (Generally not of great interest or directly interpreted)

2. age: The average change in the natural log of the odds of `elder mistreatment` for each one-year increase in age is 0.02.

```{r}
glm(
  em_f  ~ age,
  family = binomial(link = "logit"),
  data   = em_dementia
) %>% 
  broom::tidy(exp = TRUE, ci = TRUE)
```

```{r}
em_age_70 <- -3.180 + (0.02127 * 70)
em_age_70
```

```{r}
em_age_71 <- -3.180 + (0.02127 * 71)
em_age_71
```

```{r}
em_age_71 - em_age_70
```

```{r}
exp(0.02127)
```

```{r}
em_dementia %>% 
  filter(age %in% c(70, 71))
```


# Poisson regression

## Number of drinks and personal problems

```{r}
set.seed(100)
n <- 100
drinks <- tibble(
  problems = sample(0:1, n, TRUE, c(.7, .3)),
  age = rnorm(n, 50, 10),
  drinks = case_when(
    problems == 0 & age > 50 ~ sample(0:5, n, TRUE, c(.8, .05, .05, .05, .025, .025)),
    problems == 0 & age <= 50 ~ sample(0:5, n, TRUE, c(.7, .1, .1, .05, .025, .025)),
    problems == 1 & age > 50 ~ sample(0:5, n, TRUE, c(.2, .3, .2, .2, .05, .05)),
    problems == 1 & age <= 50 ~ sample(0:5, n, TRUE, c(.3, .1, .1, .2, .2, .1)),
  )
)

drinks
```

### Count regressand and continuous regressor

```{r}
mean(drinks$drinks)
```

```{r}
ggplot(drinks, aes(x = age, y = drinks)) +
  geom_point() 
```

```{r}
ggplot(drinks, aes(x = age, y = drinks)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

```{r}
glm(
  drinks  ~ age,
  family  = poisson(link = "log"),
  data    = drinks
)
```

#### Interpretation

1. Intercept: The natural log of the mean number of `drinks` when `age` is equal to zero is 0.113739. 

2. age: The average change in the natural log of the mean number of `drinks` for each one-year increase in `age` is -0.001131. 

```{r}
glm(
  drinks  ~ age,
  family  = poisson(link = "log"),
  data    = drinks
) %>% 
  broom::tidy(exp = TRUE)
```

3. Participants reported 0.9 times the number of drinks at their last drinking episode for each one-year increase in age.

```{r}
drinks_age_50 <- 0.113739 + (-0.001131 * 50)
drinks_age_50
```

```{r}
drinks_age_51 <- 0.113739 + (-0.001131 * 51)
drinks_age_51
```

```{r}
drinks_age_51 - drinks_age_50
```

```{r}
exp(-0.001131)
```

This is an incidence rate ratio. Remember that a rate is just the number of events per some specified period of time. In this case, the specified period of time is the same for all participants, so the “missing” time cancels out, and we get a rate ratio.

What's our null value for a rate ratio?

### Count regressand categorical regressor

```{r}
mean(drinks$drinks)
```

```{r}
ggplot(drinks, aes(x = factor(problems), y = drinks)) +
  geom_point() 
```

```{r}
ggplot(drinks, aes(x = factor(problems), y = drinks)) +
  geom_jitter(width = 0, height = 0.25, alpha = .5) 
```

```{r}
glm(
  drinks  ~ problems,
  family  = poisson(link = "log"),
  data    = drinks
)
```

#### Interpretation

1. Intercept: The natural log of the mean number of `drinks` when `problems` is equal to zero is -0.3848. 

2. problems: The average change in the natural log of the mean of `drinks` when `problems` changes from zero to one.

```{r}
glm(
  drinks  ~ problems,
  family  = poisson(link = "log"),
  data    = drinks
) %>% 
  broom::tidy(exp = TRUE)
```

3. Participants reporting that they had personal problems, drank 2.99 times more drinks at their last drinking episode compared to those who reported no personal problems.

This is an incidence rate ratio. Remember that a rate is just the number of events per some specified period of time. In this case, the specified period of time is the same for all participants, so the “missing” time cancels out, and we get a rate ratio.

